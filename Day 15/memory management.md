# Memory Management

The main purpose of a computer system is to execute programs. The program should be in primary memory to get executed. To improve both the utilization of the CPU and the speed of its response to users, the computer must keep several processes in main memory. The main motivation for management of main memory comes from the support for multiprogramming. Several executables processes reside in main memory at any given time. In other words, there are several programs using the main memory as their address space. Also, programs move into, and out of, the main memory as they terminate, or get suspended for some-IQ, or new executables are required to be loaded in main memory. So, the OS has to have some strategy for main memory management. Many memory-management schemes exist, reflecting various approaches, and the effectiveness of each algorithm depends on the situation. Selection of a memory-management scheme for a system depends on many factors, especially on the hardware design of the system. Each algorithm requires its own hardware support.

Also, a user requires revisiting his programs often during its evolution/However, due to the fact that primary memory is volatile, a user needs to store his program in some non-volatile store. All computers provide a non-volatile secondary memory available as an online storage. Programs and files may be disk resident and downloaded whenever their execution is required. Therefore. some form of memory management is needed at both primary and secondary memory levels. Secondary memory may store program script executable process images aped data files It may store applications, as well as, system programs. In fact, a good part of all OS, the system programs which provide services (the utilities for instance) are stored in the secondary memory. These are requisitioned as needed.

## Program Execution

Memory consists of a large array of words or bytes, each with its own address. The CPU fetches instructions from memory according to the value of the program counter. These instructions may cause additional loading from and storing to specific memory addresses,

A typical instruction-execution cycle, for example, first fetches an instruction from memory The instruction is then decoded and may cause operands to be fetched from memory. After the instruction has been executed on the operands, results may be stored back in memory. The memory unit sees only a stream of memory addresses. These addresses are generated by the instruction counter, indexing, indirection, literal addresses, and so on. Main memory and the registers built into the processor itself are the only storage that the CPU can access directly. There are machine instructions that take memory addresses as arguments, but none that take disk addresses. Therefore, any instructions in execution, and any data being used by the instructions, must be in one of these direct-access storage devices. If the data are not in memory, they must be moved there before the CPU can operate on them.

## Address Binding

Usually, a program resides on a disk as a binary executable file. To be executed, the program must be brought into memory and placed within a process. Depending on the memory management in use, the process may be moved between disk and memory during its execution. The processes on the disk that are waiting to be brought into memory for execution form the input queue. The normal procedure is to select one of the processes in the input queue and to load that process into memory. As the process is executed, it accesses instructions and data from memory. Eventually, the process terminates, and its memory space is declared available.

A user program will go through several steps optional before being executed. Addresses may be represented in different ways during these steps. Addresses in the source program are generally symbolic (such as count). A compiler will typically bind these symbolic addresses to relocatable addresses (these are relative addresses). The linkage editor or loader will in turn bind the relocatable addresses to absolute addresses. Each binding is a mapping from one address space to another. Classically, the binding of instructions and data to memory addresses can be done at any step along the way:

Thank

1) Compile time: If you know at compile time where the process will reside in memory. then absolute code can be generated. For example, if you know that a user process will reside starting at location R, then the generated compiler code will start at that location and extend up from there. If, at some later time, the starting location changes, then it will be necessary to recompile this code.

2) Load time. If it is not known at compile time where the process will reside in memory. then the compiler must generate relocatable code. In this case, final binding is delayed until load time. If the starting address changes, you need only reload the user code to incorporate this changed value.

3) Execution time. If the process can be moved during its execution from one memory A segment to another, then binding must be delayed until run time. Special hardware must be available for this scheme to work. Most general-purpose operating systems use this method.